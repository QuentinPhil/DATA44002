{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HW8 - Twitter Clustering\n",
        "### Quentin Phillips\n",
        "### DATA 440 Fall 2023\n",
        "### 12/8/23\n",
        "\n",
        "# Q1- My List\n",
        "\n",
        "#### I saved 50 twitter user tags in the file 'accounts.txt' pertaining to one of three categories: sports, gaming, and music. The accounts range in sizes, but all meet the required significance statistics. I picked a few outliers and a few that are connected to the categories, but not strictly a part of them. I'm interested to see where these fall once I run the clustering algorithm.\n",
        "\n",
        "#### My prediction for the final list with k=5 is:\n",
        "\n",
        "####Athlete\n",
        "####Musician\n",
        "####EsportsOrg\n",
        "####Influencer\n",
        "####SportsNews\n"
      ],
      "metadata": {
        "id": "Us6ergXbmn9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2\n",
        "\n",
        "## A\n",
        "\n",
        "#### The generate_tweet_vector.py file has many different functions that it executes together. The main 3 purposes are to: scrape tweets from the accounts specified, take the content of those tweets and break it into separate meaningful words, and then take that data and form a matrix with it that can be used to analyze the context of the tweets.\n",
        "\n",
        "## B\n",
        "\n",
        "#### The code I used to filter the tweets is pretty short, but should work without any issues. First, I sort the dictionary by the value of each key; this means that the most common words are at the front of the dictionary, and the least common at the end. With this structure in place, I can iterate through the dictionary and set the range to 500 with each iteration adding the key to the given empty list. This means the top 500 keys will be stored in the list. I see a potential issue with tied word counts, but this should not be a problem as if the word counts are tied near the 500 mark, I would need to use my own discretion anyway to decide which to include.\n",
        "\n",
        "## C\n",
        "\n",
        "#### The top 500 words are in line with what I was expecting. To no surprise \"the\" was the most common word, and many of the top words included positive messaging like \"yeah\" or \"win\" or \"champion\". Because I chose many musicians and athletes, this was what I expected to encounter.\n"
      ],
      "metadata": {
        "id": "Qvx0Gb-ganvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3\n",
        "\n",
        "#### Below is the jpeg of the dendrogram:"
      ],
      "metadata": {
        "id": "4DCl_jq5O4D6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=13hgF9ZfzpXLUUIR8GGNLwOMoQSwOQx-k)\n"
      ],
      "metadata": {
        "id": "MNEb_1uDNtjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This dendrogram has a lot of weird groupings. Some organizations from completely different parts of the world and in completely different fields got grouped together, such as TTNGuk (small english band) and TSM (huge US gaming org). It's interesting to me how little the groups clustered that I expected to, maybe because of problems with my data collection."
      ],
      "metadata": {
        "id": "IXtbn7_cPMgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4\n",
        "\n",
        "##A\n",
        "\n",
        "#### This Kmeans algorithm functions by finding values in each row of the data array and then comparing them to randomly generated centroids. Then the centroids shift to reflect the placement of the data points that are closest to that centroid. Several iterations are run, and then a final set of matches is produced.\n",
        "\n",
        "## B\n",
        "\n",
        "#### For this run of the algorithm, it took 4 runs for k=5, 3 runs for k=10, and 4 runs for k=20. I was expecting a more linear relationship between k and the number of iterations, but that is not how it worked out.\n",
        "\n",
        "## C\n",
        "\n",
        "#### K means 5, by far, created the most accurate clustering. The first cluster was the largest, and consisted primarily of individual athletes, musicians, and artists. Clusters 2 and 4 were both only 1 account. The 5th cluster consisted of primarily large organizations, and most of the sports news organizations. This clustering is not too surprising for me, but did cluster individuals and groups seperately more than it clustered them by the nature of their work, which is not something I anticipated."
      ],
      "metadata": {
        "id": "iruBZnozPM2S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h14xBzYUQC8J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}