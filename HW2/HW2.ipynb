{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us6ergXbmn9T"
      },
      "source": [
        "# HW2 - Scraping X and Creating TimeMaps\n",
        "### Quentin Phillips\n",
        "### DATA 440 Fall 2023\n",
        "### 10/26/23\n",
        "\n",
        "# 1. Collect URIs from tweets\n",
        "\n",
        "## Below is the modified code that I used to scrape the base URLs. These URLs still required processing, which I used a separate function for which I will include later. I saved this as a python file in VScode and then ran it through windows powershell via >python3 \"C:\\\\Users\\\\quill\\\\Downloads\\\\scrape_twitter_v2.py\"\n",
        "\n",
        "##I had to run this code several times to get many links. I attempted to scrape 1000 tweets per keyword, and the keywords I used included: Taylor Swift, Breaking News, Gaza, Halloween, etc.\n",
        "## This process took longer than I anticipated, and in the future I would run it more times to collect well over 1000 links so that after cleaning and removing some links I still have sufficient numbers.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JurpXBlhkHJQ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import time\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from NwalaTextUtils.textutils import genericErrorInfo\n",
        "from NwalaTextUtils.textutils import getLinks\n",
        "\n",
        "from playwright.sync_api import sync_playwright\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "from util import paral_rehydrate_tweets\n",
        "from util import rehydrate_tweet\n",
        "from util import write_tweets_to_jsonl_file\n",
        "\n",
        "from scrape_twitter import get_auth_twitter_pg\n",
        "from scrape_twitter import get_search_tweets\n",
        "#from scrape_twitter import get_timeline_tweets\n",
        "from util import write_tweets_to_jsonl_file\n",
        "\n",
        "import gzip\n",
        "import json\n",
        "\n",
        "\n",
        "def is_twitter_user_auth(links, cur_page_uri):\n",
        "\n",
        "    if( cur_page_uri.strip().startswith('https://twitter.com/home') ):\n",
        "        return True\n",
        "\n",
        "    logged_in_links = ['https://twitter.com/home', 'https://t.co/']\n",
        "\n",
        "    for l in links:\n",
        "        for log_l in logged_in_links:\n",
        "            if( l['link'].startswith(log_l) ):\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def scroll_up(page):\n",
        "    page.evaluate(\"window.scrollTo( {'top': 0, 'left': 0, 'behavior': 'smooth'} );\")\n",
        "\n",
        "def scroll_down(page):\n",
        "    page.evaluate(\"window.scrollTo( {'top': document.body.scrollHeight, 'left': 0, 'behavior': 'smooth'} );\")\n",
        "\n",
        "def post_tweet(page, msg, button_name='Post', after_post_sleep=2.5):\n",
        "    #Post, Reply\n",
        "    eval_str = f''' document.querySelectorAll('[aria-label$=\"{button_name}\"]')[0].click(); '''\n",
        "    page.evaluate(eval_str)\n",
        "    time.sleep(1)\n",
        "    page.keyboard.type(msg, delay=20)\n",
        "    page.evaluate(''' document.querySelectorAll('[data-testid=\"tweetButton\"]')[0].click(); ''')\n",
        "    time.sleep(after_post_sleep)\n",
        "\n",
        "\n",
        "def color_tweet(page, tweet_link):\n",
        "\n",
        "    query_slc = f'''article = document.querySelectorAll('[href=\"{tweet_link}\"]');'''\n",
        "    page.evaluate(query_slc + '''\n",
        "        if( article.length != 0 )\n",
        "        {\n",
        "            article = article[0];\n",
        "            article.style.backgroundColor = 'red';\n",
        "            i = 0;\n",
        "            while(i < 1000)\n",
        "            {\n",
        "                if( article.nodeName == 'ARTICLE' )\n",
        "                {\n",
        "                    article.style.outline = \"thick solid red\";\n",
        "                    article.className = \"cust-tweet\";\n",
        "                    break;\n",
        "                }\n",
        "                article = article.parentElement;\n",
        "                i++;\n",
        "            }\n",
        "        }\n",
        "    ''')\n",
        "\n",
        "\n",
        "def get_tweet_ids_user_timeline_page(screen_name, page, max_tweets):\n",
        "\n",
        "    prev_len = 0\n",
        "    empty_result_count = 0\n",
        "\n",
        "    tweet_links = set()\n",
        "    break_flag = False\n",
        "\n",
        "    while( True ):\n",
        "\n",
        "        page_html = page.content()\n",
        "        soup = BeautifulSoup(page_html, 'html.parser')\n",
        "        articles = soup.find_all('article')\n",
        "\n",
        "        for i in range(len(articles)):\n",
        "\n",
        "            t = articles[i]\n",
        "            is_retweet = t.find('span', {'data-testid': 'socialContext'})\n",
        "            is_retweet = False if is_retweet is None else is_retweet.text.strip().lower().endswith(' retweeted')\n",
        "\n",
        "            tweet_datetime = ''\n",
        "            tweet_link = t.find('time')\n",
        "\n",
        "            if( tweet_link is None ):\n",
        "                tweet_link = ''\n",
        "            else:\n",
        "                tweet_datetime = tweet_link.get('datetime', '')\n",
        "                tweet_link = tweet_link.parent.get('href', '')\n",
        "\n",
        "            if( tweet_link == '' ):\n",
        "                continue\n",
        "\n",
        "\n",
        "            if( screen_name != '' and is_retweet is False and tweet_link.startswith(f'/{screen_name}/') is False ):\n",
        "                #This tweet was authored by someone else, NOT the owner of the timeline, and since it was not retweeted\n",
        "                continue\n",
        "\n",
        "            #color_tweet(page, tweet_link)\n",
        "            tweet_links.add( tweet_link )\n",
        "            twt_id = tweet_link.split('/status/')[-1]\n",
        "            tweet_obj = rehydrate_tweet(twt_id)\n",
        "            if( len(tweet_obj) == 0 ):\n",
        "                print('\\tOOPS! rehydration failed! patch rehydrate_tweet()')\n",
        "                continue\n",
        "\n",
        "            print( '\\ttweets {}, datetime: {}, is_retweet: {}'.format(len(tweet_links), tweet_datetime, is_retweet) )\n",
        "            #print( f'\\tdo stuff here with tweet_obj: {tweet_obj.keys()}\\n' )\n",
        "\n",
        "\n",
        "            #file_path = \"C:\\\\Users\\\\quill\\\\Downloads\\\\linksfile.txt\"\n",
        "            if 'urls' in tweet_obj.get('entities', []):\n",
        "                for link in tweet_obj['entities']['urls']:\n",
        "                    print(link['expanded_url'])\n",
        "                    if 'twitter.com' not in link['expanded_url'] and 'youtu.be' not in link['expanded_url'] and 'soundcloud.com' not in link['expanded_url'] and 'twitch.com' not in link['expanded_url']and 'youtube.com' not in link['expanded_url']:\n",
        "                        with open(\"C:\\\\Users\\\\quill\\\\Downloads\\\\linksfile.txt\", 'a') as file:\n",
        "                            file.writelines(link['expanded_url'] + '\\n')\n",
        "\n",
        "            if( len(tweet_links) == max_tweets ):\n",
        "                print(f'exiting reached ({len(tweet_links)}) maximum: {max_tweets}')\n",
        "                sys.exit(0)\n",
        "\n",
        "        empty_result_count = empty_result_count + 1 if prev_len == len(tweet_links) else 0\n",
        "        if( empty_result_count > 5 ):\n",
        "            print(f'No new tweets found, so exiting')\n",
        "            sys.exit(0)\n",
        "\n",
        "        prev_len = len(tweet_links)\n",
        "        print('\\tthrottling/scrolling, then sleeping for 2 second\\n')\n",
        "        scroll_down(page)\n",
        "        time.sleep(2)\n",
        "\n",
        "\n",
        "def get_timeline_tweets(browser_dets, screen_name, max_tweets=20):\n",
        "\n",
        "    screen_name = screen_name.strip()\n",
        "    if( max_tweets < 0  or len(browser_dets) == 0 or screen_name == '' ):\n",
        "        return {}\n",
        "\n",
        "    print( f'\\nget_timeline_tweets(): {screen_name}' )\n",
        "    uri = f'https://twitter.com/{screen_name}/with_replies'\n",
        "\n",
        "    payload = {'self': uri, 'tweets': []}\n",
        "    browser_dets['page'].goto(uri)\n",
        "\n",
        "    tweet_ids = get_tweet_ids_user_timeline_page( screen_name, browser_dets['page'], max_tweets )\n",
        "    payload['tweets'] = paral_rehydrate_tweets(tweet_ids)\n",
        "\n",
        "    return payload\n",
        "\n",
        "def stream_tweets(browser_dets, query, max_tweets=20):\n",
        "\n",
        "    query = query.strip()\n",
        "    if( max_tweets < 0  or len(browser_dets) == 0 or query == '' ):\n",
        "        return {}\n",
        "\n",
        "    print('\\nstream_tweets():')\n",
        "    uri = 'https://twitter.com/search?q=' + quote_plus(query) + '&f=live&src=typd'\n",
        "\n",
        "    payload = {'self': uri, 'tweets': []}\n",
        "    browser_dets['page'].goto(uri)\n",
        "\n",
        "    get_tweet_ids_user_timeline_page( '', browser_dets['page'], max_tweets )\n",
        "\n",
        "def get_auth_twitter_pg(playwright, callback_uri=''):\n",
        "\n",
        "    print('\\nget_auth_twitter_pg()')\n",
        "\n",
        "    chromium = playwright.firefox #\"chromium\" or \"firefox\" or \"webkit\".\n",
        "    browser = chromium.launch(headless=False)\n",
        "    context = browser.new_context()\n",
        "    page = context.new_page()\n",
        "\n",
        "    sleep_seconds = 3\n",
        "    page.goto('https://twitter.com/login')\n",
        "\n",
        "    while( True ):\n",
        "\n",
        "        print(f'\\twaiting for login, sleeping for {sleep_seconds} seconds')\n",
        "\n",
        "        time.sleep(sleep_seconds)\n",
        "        page_html = page.content()\n",
        "        page_links = getLinks(uri='', html=page_html, fromMainTextFlag=False)\n",
        "        scroll_down(page)\n",
        "\n",
        "        if( is_twitter_user_auth(page_links, page.url) ):\n",
        "\n",
        "            print('\\tauthenticated')\n",
        "            if( callback_uri != '' ):\n",
        "                page.goto(callback_uri)\n",
        "                print(f'\\tauthenticated, loaded {callback_uri}')\n",
        "\n",
        "            print('\\tsleeping for 3 seconds')\n",
        "            time.sleep(3)\n",
        "            return {\n",
        "                'page': page,\n",
        "                'context': context,\n",
        "                'browser': browser\n",
        "            }\n",
        "\n",
        "    return {}\n",
        "\n",
        "def main():\n",
        "\n",
        "    if( len(sys.argv) != 3 ):\n",
        "        print(f'Usage example:\\n\\tpython {sys.argv[0]} \"williamsburg\" 20')\n",
        "        return\n",
        "\n",
        "    with sync_playwright() as playwright:\n",
        "\n",
        "        browser_dets = get_auth_twitter_pg(playwright)\n",
        "        if( len(browser_dets) == 0 ):\n",
        "            return\n",
        "\n",
        "        stream_tweets(browser_dets, sys.argv[1], max_tweets=int(sys.argv[2]))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4LfHflVoloA"
      },
      "source": [
        "## This code returned a .txt file with many links that included redirects or youtube links. To remove the youtube links, I added the \"'youtube.com' not in link['expanded_url']:\" and \"'youtu.be' not in link['expanded_url']: lines to the code to exclude these video links.\n",
        "\n",
        "##Next I ran the following python script in PowerShell to return the processed URIs file. I ensured that the saved links returned a HTTP 200 status code to make sure they are not redirecting.\n",
        "\n",
        "##I ran this with >Python3 \"C:\\\\Users\\\\quill\\\\Downloads\\\\URI_Retriever.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDU5odVmqY-a"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def resolve_urls(url_list, timeout=10):\n",
        "    resolved_urls = set()\n",
        "\n",
        "    for url in url_list:\n",
        "        try:\n",
        "            response = requests.head(url, allow_redirects=True, timeout=timeout)\n",
        "            if response.status_code == 200:\n",
        "                resolved_url = response.url\n",
        "                resolved_urls.add(resolved_url)\n",
        "            else:\n",
        "                print(f\"Skipping non-200 URL: {url}\")\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"Skipping timed out URL: {url}\")\n",
        "        except requests.exceptions.RequestException:\n",
        "            print(f\"Failed to connect: {url}\")\n",
        "\n",
        "    return resolved_urls\n",
        "\n",
        "def read_urls(input_file_path):\n",
        "    unique_urls = set()\n",
        "\n",
        "    with open(input_file_path, \"r\") as input_file:\n",
        "        for line in input_file:\n",
        "            url = line.strip()\n",
        "            unique_urls.add(url)\n",
        "\n",
        "    return list(unique_urls)\n",
        "\n",
        "input_file_path = \"C:\\\\Users\\\\quill\\\\Downloads\\\\linksfile.txt\"\n",
        "output_file_path = \"C:\\\\Users\\\\quill\\\\Downloads\\\\resolvedURLS.txt\"\n",
        "\n",
        "unique_urls = read_urls(input_file_path)\n",
        "resolved_urls = resolve_urls(unique_urls, timeout=10)\n",
        "\n",
        "# Save the resolved URLs to resolvedURLS.txt\n",
        "with open(output_file_path, \"w\") as output_file:\n",
        "    for url in resolved_urls:\n",
        "        output_file.write(url + \"\\n\")\n",
        "\n",
        "print(f\"Resolved URLs saved to {output_file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw4EHUa4qpmE"
      },
      "source": [
        "##This returned the resolvedURLS.txt file that I then used through MemGator to create the TimeMaps. I have included the .txt file in this repo.\n",
        "\n",
        "# 2. Generate TimeMaps\n",
        "## I used Python to automate running the links through MemGator. The script I used is included below with some notes about switching to Linux:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmqqvEcEsoHp"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "file_path = \"C:\\\\Users\\\\quill\\\\Downloads\\\\resolvedURLs.txt\"\n",
        "#Changed the binary path when running on Linux\n",
        "memgator_binary = \"C:\\\\Users\\\\quill\\\\Downloads\\\\memgator-windows-amd64.exe\"\n",
        "\n",
        "with open(file_path, \"r\") as uri_file:\n",
        "    for line in uri_file:\n",
        "        uri = line.strip()\n",
        "\n",
        "        # Ended up not using WSL because I had to switch to Linux machine, but still including because this is intended for PowerShell\n",
        "        cmd = [\"wsl\", memgator_binary, \"--format=JSON\", uri]\n",
        "\n",
        "        try:\n",
        "            subprocess.run(cmd, check=True)\n",
        "            print(f\"TimeMap Generated for: {uri}\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            # Error handling\n",
        "            print(f\"Error processing URI: {uri}\")\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "print(\"All TimeMaps Generated.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzG0b9VNkw5X"
      },
      "source": [
        "# 3. Memento Table\n",
        "\n",
        "|Mementos|URI-Rs|\n",
        "|:---|:---|\n",
        "|0|0|\n",
        "|1-5|0|\n",
        "|6-10|0|\n",
        "|11-15|0|\n",
        "|16-20|0|\n",
        "\n",
        "#Extra Credit\n",
        "\n",
        "https://conifer.rhizome.org/qdphillips/volleyball-history\n",
        "\n",
        "Q1. Volleyball is a big part of my life but it is a relatively small sport. I wanted to get a sense of how well documented professional results are and do my part to archive them.\n",
        "\n",
        "Q2. Archiving these webpages was actually a very smooth experience using conifer through google chrome.\n",
        "\n",
        "Q3. The archived pages look very similar to the original, even with the complex formatting.\n",
        "\n",
        "Q4. The archive saved 100 URLs. Way more than I was anticipating.\n",
        "\n",
        "![pic](https://github.com/QuentinPhil/DATA44002/blob/main/HW2/Screenshot 2023-10-26 154510.png?raw=true)\n",
        "\n",
        "#References\n",
        "###(Many not directly relevant, but I needed to debug/learn about linux and command line)\n",
        "\n",
        "* Insert Reference 1, <https://www.geeksforgeeks.org/reading-and-writing-json-to-a-file-in-python/>\n",
        "* Insert Reference 2, <https://www.annasyme.com/docs/python-subprocess.html>\n",
        "* Insert Reference 3, <https://askubuntu.com/questions/588390/how-do-i-check-whether-a-module-is-installed-in-python-and-install-it-if-needed>\n",
        "* Insert Reference 4, <https://askubuntu.com/questions/1309941/what-does-a-no-such-file-or-directory-mean-and-how-do-i-correct-the-problem/reallyreallyreally-extra-long-URI/>\n",
        "* Insert Reference 5, <https://unix.stackexchange.com/questions/166728/chmod-cannot-access-file-no-such-file-or-directory-error-when-the-file-exis>\n",
        "* Insert Reference 6, <https://learn.microsoft.com/en-us/windows/wsl/install-manual#step-6---install-your-linux-distribution-of-choice>\n",
        "* Insert Reference 7, <https://chat.openai.com/>\n",
        "* Insert Reference 8, <https://learn.microsoft.com/en-us/windows/wsl/basic-commands#unregister-or-uninstall-a-linux-distribution>\n",
        "* Insert Reference 9, <https://requests.readthedocs.io/en/latest/>\n",
        "* Insert Reference 10, <https://stackoverflow.com/questions/25651990/oserror-winerror-193-1-is-not-a-valid-win32-application>\n",
        "* Insert Reference 11, <https://opensource.com/article/22/7/use-bash-automate-tasks-linux>\n",
        "* Insert Reference 12, <https://stackoverflow.com/questions/4278083/how-to-get-request-uri-without-context-path>\n",
        "* Insert Reference 13, <https://stackoverflow.com/questions/44327037/read-links-from-a-list-from-a-txt-file-python>\n",
        "* Insert Reference 14, <https://www.geeksforgeeks.org/get-method-python-requests/>\n",
        "* Insert Reference 15, <https://stackoverflow.com/questions/46783078/uri-encoding-in-python-requests-package>\n",
        "* Insert Reference 16, <https://unix.stackexchange.com/questions/238180/execute-shell-commands-in-python>\n",
        "* Insert Reference 17, <https://realpython.com/python-subprocess/>\n",
        "* Insert Reference 18, <https://www.tutorialspoint.com/python/python_command_line_arguments.htm>\n",
        "\n",
        ">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot1XYMjVkyhY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
